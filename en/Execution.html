<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6. Runtime options and behavior &#8212; Singularity on FUGAKU  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="7. Cooperation with job scheduler" href="Submitting.html" />
    <link rel="prev" title="5. Creating an image" href="Creating.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="id1">
<h1><span class="section-number">6. </span>Runtime options and behavior<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h1>
<p>Here, we will introduce some options that are often used when running Singularity, and some useful behaviors.</p>
<section id="id2">
<h2><span class="section-number">6.1. </span>Incorporating the environment on the host side<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h2>
<section id="id3">
<h3><span class="section-number">6.1.1. </span>Handling of environment variables<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<p>We have seen that access to files in your home directory is enabled by default. What about environment variables? If you display the environment variables in the container as follows, you can see that the variables on the host side at runtime are inherited.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec ubuntu2004.sif env
</pre></div>
</div>
<p>The content of %environment described in the definition file is actually posted as a script in the image, and the operation that is sourced when the container is started. Therefore, it is possible to capture and manipulate variables on the host side or overwrite them. Also, if you set a variable called SINGULARITYENV_*** (*** is an arbitrary string that can be used for variables) on the host side before executing the container, since the variable *** will be set in the container, the variable in the container is set in advance without interfering with the environment on the host side. And the content described in %environment is temporarily set or overwritten without recreating the image.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat example.def
&lt;&lt;..snip..&gt;&gt;
%environment
    export WORKFILE=/tmp/work_defined_in_env
%runscript
    echo &#39;WORKFILE in Container&#39; = ${WORKFILE}

$ cat job.sh
&lt;&lt;..snip..&gt;&gt;
    export WORKFILE=/workdir/workfile
    export SINGULARITYENV_WORKFILE=/workdir/work_defined_by_SINGULARITYENV
    ~/example.sif
</pre></div>
</div>
<p>The execution result in this case is as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">WORKFILE</span> <span class="ow">in</span> <span class="n">Container</span> <span class="o">=</span> <span class="o">/</span><span class="n">workdir</span><span class="o">/</span><span class="n">work_defined_by_SINGULARITYENV</span>

<span class="c1"># without setting SINGULARITYENV</span>
<span class="n">WORKFILE</span> <span class="ow">in</span> <span class="n">Container</span> <span class="o">=</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">work_defined_in_env</span>

<span class="c1"># without setting both of SINGULARITYENV and %environment</span>
<span class="n">WORKFILE</span> <span class="ow">in</span> <span class="n">Container</span> <span class="o">=</span> <span class="o">/</span><span class="n">workdir</span><span class="o">/</span><span class="n">workfile</span>
</pre></div>
</div>
<p>This makes it easy to change conditions in different environments without having to recreate the image.</p>
</section>
<section id="id4">
<h3><span class="section-number">6.1.2. </span>Directory binding<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h3>
<p>When you start a singularity container, it binds your own home directory, /tmp, /var/tmp, /dev, /proc, /sys. inside the container by default. It also binds the current directory at startup, but only if a directory with the same name exists in the container. If you boot the image from a directory that does not exist in the container, you may want to access other directories from within the container, as well as the current directory. For example, applications and data already installed on the host side may be redundant or huge to be imported into the image, or you may want to use the home directory of another user who shares the file.</p>
<p>In this case, you can specify additional directories to bind with the -B option at boot time. you can specify different paths by separating the directory on the host side and the mount point in the container with ':' , but if you omit the mount destination, it will be bound to the directory with the same name.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec ubuntu2004.sif -B /worktmp:/scratch ls -al /scratch
</pre></div>
</div>
<p>It will be remounted inside the container with the -bind option, but you can add more mount options here with ':' . For example, if you want to make it read-only in a container, you can write as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec ubuntu2004.sif -B /system/reference:/opt/data:ro ls -al /opt/data
</pre></div>
</div>
<p>Also, if you want to specify multiple directories, you can use multiple -B or separate them with ','.</p>
<p>There are some caveats to the binding behavior. The current directory is bound by default, so if you start a container from a directory such as /usr/ bin or /usr/lib, it will replace them in the container with the directory on the host side, resulting in a runtime error. There is. also, if you start from another user’s home directory and try to bind automatically, it will not be bound because that directory does not exist in the container. Must be explicitly specified with the -B option.</p>
</section>
</section>
<section id="id5">
<h2><span class="section-number">6.2. </span>Instantiation and interactive processing<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h2>
<p>So far, we have integrated the launch of the container and the launch of the application and used it so that the user can launch the application directly without being aware of the container. However, with this usage, when the started application is closed, the container will be closed at the same time. Of course, you can restart it, but even if it is a lightweight container implementation, if you repeat it many times, the overhead will be wasted. Also, can singularity not be used for service operations such as web servers? To accommodate these uses, singularity has a feature called instantiation that allows you to maintain containers.</p>
<p>Start and instantiate only the container as shown below. The last string is the instance name, which must be a name that is not covered by what is already running. Make sure it is started correctly with the list subcommand.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity instance start ubuntu2004.sif Focal
$ singularity instance list
INSTANCE NAME    PID      IP    IMAGE
Focal            12016          /home/uXXXXX/ubuntu2004.sif
</pre></div>
</div>
<p>The image is now unpacked and mounted, that is, the container is up and waiting. To run your application within this instance, specify the instance name instead of the image name, as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec instance://Focal cat /etc/os-release
</pre></div>
</div>
<p>Multiple applications can be submitted and executed on a single instance. For example, suppose you need to perform the same process on a huge number of files in a certain directory.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>for data in data.*
do
    singularity exec jammy.sif appl ${data}
done
</pre></div>
</div>
<p>While it is tempting to write such a script, the following will generally reduce the container launch overhead and thus increase throughput.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>singularity instance start jammy jammy.sif
for data in data.*
do
    singularity exec instance://jammy appl ${data}
done
</pre></div>
</div>
<p>Since you can start multiple instances simultaneously, you can perform different operations in different environments in single job.</p>
<p>To terminate running instance, use stop option.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity instance stop Focal
</pre></div>
</div>
</section>
<section id="id6">
<h2><span class="section-number">6.3. </span>Direct execution with specified repository<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h2>
<p>Recall that we were getting a docker image when we created the sif image. during pull and build, the layer files that make up the docker image are cached under ~/.singularity. The behavior of this cache does not change even if you specify oras or library as the target. And singularity can do this directly.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec docker://ubuntu:latest head -n 5 /etc/os-release
INFO:    Converting OCI blobs to SIF format
WARNING: &#39;nodev&#39; mount option set on /worktmp, it could be a source of failure during build process
INFO:    Starting build...
Getting image source signatures
Copying blob a39c84e173f0 done
Copying config 4db8450a4d done
Writing manifest to image destination
Storing signatures
2021/03/21 12:14:15  info unpack layer: sha256:a39c84e173f038958d338f55a9e8ee64bb6643e8ac6ae98e08ca65146e668d86
2021/03/21 12:14:15  warn xattr{etc/gshadow} ignoring ENOTSUP on setxattr &quot;user.rootlesscontainers&quot;
2021/03/21 12:14:15  warn xattr{/worktmp/build-temp-063655636/rootfs/etc/gshadow} destination filesystem does not support xattrs, further warnings will be suppressed
INFO:    Creating SIF file...
NAME=&quot;Ubuntu&quot;
VERSION=&quot;20.04.3 LTS (Focal Fossa)&quot;
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=&quot;Ubuntu 20.04.3 LTS&quot;
</pre></div>
</div>
<p>As you can see from the message, it is internally sif imaged and stored under ~/.singularity. when you re-execute, it will check the repository for updates, make the best use of the cache, and synchronize. If there is no update, launch his sif image in the cache directly as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec docker://ubuntu:latest head -n 5 /etc/os-release
INFO:    Using cached SIF image
NAME=&quot;Ubuntu&quot;
VERSION=&quot;20.04.3 LTS (Focal Fossa)&quot;
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=&quot;Ubuntu 20.04.3 LTS&quot;
</pre></div>
</div>
<p>If there is an update on the repository side, get it, recreate the sif image, and then execute it. This execution method is not suitable when it is necessary to maintain an execution environment with a proven track record. It is recommended that you always use it only when you want to import and use the latest version of the repository.</p>
<p>You can also view cache information and clear cache files.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity cache list -v
NAME                     DATE CREATED           SIZE             TYPE
061be60b872e3a155a2364   2021-10-28 22:14:15    0.34 KiB         blob
2d5d59c100e7ad4ac35a22   2021-11-03 13:54:19    78.57 MiB        blob
&lt;&lt;..snip..&gt;&gt;
sha256.9a6ee1f8fdecb21   2023-02-22 17:14:02    2.65 MiB         library
626ffe58f6e7566e00254b   2021-10-28 22:14:18    25.15 MiB        oci-tmp

There are 2 container file(s) using 27.81 MiB and 25 oci blob file(s) using 692.86 MiB of space
Total space used: 720.67 MiB

$ singularity cache clean
</pre></div>
</div>
<p>These caches are used not only at runtime, but also at build/pull time.</p>
</section>
<section id="overlay">
<h2><span class="section-number">6.4. </span>Overlay option<a class="headerlink" href="#overlay" title="Link to this heading">¶</a></h2>
<p>The SIF image entity is SquashFS(almost tar+gzip), so it is read-only at runtime. However, there are cases when you want to temporarily place files in the image, or when you need an image that is only partially different, or when you want to have only the differences because having multiple images that contain everything is a waste of capacity. Singularity provides 2 solutions.</p>
<ol class="arabic simple">
<li><p>The –writable-tmpfs option that makes memory-derived tmpfs temporarily writable by superimposing them on the read-only image.</p></li>
<li><p>The –overlay option that prepares a separate persistent writable file and overlays it at runtime.</p></li>
</ol>
<p>The writable-tmpfs option superimposes tmpfs on the existing image, so new files and directories can be created at any location in container. However, files originally in the image cannot be manipulated with causing the error of Permission denied. Also, the changes will only exist in tmpfs, so they will disappear after the container is terminated. If you have necessary data or files, save them separately before exiting the container.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity shell -writable-tmpfs something.sif
Singularity&gt; echo hogefuga &gt; /etc/testfile
Singularity&gt; ls -al /etc/testfile
-rw-rw-r-- 1 uXXXXX fugaku 9 Feb 14 04:50 /etc/testfile
Singularity&gt; echo hogefuga &gt; /etc/os-release
bash: /etc/os-release: Permission denied
Singularity&gt; exit

$ singularity shell something.sif
Singularity&gt; ls -al /etc/hogefuga
ls: cannot access &#39;/etc/hogefuga&#39;: No such file or directory
</pre></div>
</div>
<p>The –overlay function, which overlays a separately prepared persistent writable file at runtime, requires a separate image file to be created instead of tmpfs. If the same file exists in both SIF and Overlay, the overlay side has priority. This means that it enables partial modification of the image.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># 1GBの overlay イメージを作成し、指定したディレクトリを予め作成しておく。
# Creating a 1GB of image file with prepared directory inside.
$ singularity overlay create --size 1024 --create-dir /usr/share/apps overlay.img

# overlay イメージ内にデータを展開
# Extracting data into Overlay image.
$ singularity exec --overlay overlay.img core.sif tar xzf reference-data.tar.gz -C /usr/share/apps

# overlay イメージをコンテナに重ね合わせて実行
# Executeing container with the overlay image.
$ singularity exec --overlay overlay.img core.sif ls -al /usr/share/apps
</pre></div>
</div>
<p>The written data remains in overlay.img and can be used to carry and segregate your data. However, please note that the image file is formatted in ext3 and the owner information is also retained as well, so be careful when using it on a different system. On the other hand, the image file can be used on both of AARCH64 and x86_64 by singularity. It is also possible to directly manipulate the image by mounting it as loopback, or to resize it in the same way as a normal partition, as shown below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ e2fsck -f overlay.img &amp;&amp; resize2fs overlay.img 4096M
</pre></div>
</div>
<p>In addition, you can integrate the overlay image into a SIF image to be a single file by means of singularity sif command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity sif list ubuarm20.sif
------------------------------------------------------------------------------
ID   |GROUP   |LINK    |SIF POSITION (start-end)  |TYPE
------------------------------------------------------------------------------
1    |1       |NONE    |65536-65574               |Def.FILE
2    |1       |NONE    |131072-131169             |JSON.Generic
3    |1       |NONE    |196608-26320896           |FS (Squashfs/*System/arm64)

$ singularity sif add --datatype 4 --partfs 2 --parttype 4 --partarch 4 ubuarm20.sif overlay.img
$ singularity sif list ubuarm20.sif
------------------------------------------------------------------------------
ID   |GROUP   |LINK    |SIF POSITION (start-end)  |TYPE
------------------------------------------------------------------------------
1    |1       |NONE    |65536-65574               |Def.FILE
2    |1       |NONE    |131072-131169             |JSON.Generic
3    |1       |NONE    |196608-26320896           |FS (Squashfs/*System/arm64)
4    |NONE    |NONE    |26320896-1100062720       |FS (Ext3/Overlay/arm64)
</pre></div>
</div>
<p>To see the meaning of each parameter, please refer to the manual which you can see by the command singularity sif add –help. This function allows for the operation of images whose contents can be modified. Later, these additions can be taken out or deleted.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity sif dump 4 ubuarm20.sif  &gt; ovl.img
$ singularity sif del 4 ubuarm20.sif
</pre></div>
</div>
<p>These operations can also be performed on the login node, so there is no need to submit a job for this purpose. Overlay also contributes to reducing the metadata access load on the shared file system, especially when there are many small files, since I/O is completed within a single file.</p>
</section>
<section id="mpi">
<h2><span class="section-number">6.5. </span>MPI parallelism<a class="headerlink" href="#mpi" title="Link to this heading">¶</a></h2>
<p>Regarding mpi parallelism, there are no major problems up to parallelism within a node, but since there are various points to be solved with multi-node parallelism, additional work and configuration is required at image creation and runtime.</p>
<section id="id7">
<h3><span class="section-number">6.5.1. </span>Parallel within a node<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h3>
<p>In parallel within a node, the process manager launches the application directly, so processing does not extend outside the container. While launching the container with singularity, launch the mpi application with mpirun (mpiexec) etc. of course, it is assumed that the required mpi runtime is in the container. An example command looks like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec mpi-apps.sif mpiexec -np 8 ~/myapps/hoge inputfile
</pre></div>
</div>
<p>Only one container is started, and multiple processes run in it.</p>
</section>
<section id="id8">
<h3><span class="section-number">6.5.2. </span>Overview of multi-node parallel<a class="headerlink" href="#id8" title="Link to this heading">¶</a></h3>
<p>When multi-node parallel application is started in the same way as parallel in a node, the process manager will be started to other nodes by ssh or PMI. However, since the container isn’t started on the destination node, the application almost always fails to start with error. Therefore, we address this by having the MPI process manager include container startup, not only application startup. An example of how to start it is as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ mpiexec -np 32 --machinefile nodefile singularity exec mpi-apps.sif ~/myapps/hoge inputfile
</pre></div>
</div>
<p>Compared to intranode parallel execution, the order of mpiexec and singularity commands is different. In this case, you need to execute mpiexec outside the Singularity container, This means that you need to ensure that the same MPI is installed both on the host side and inside the container, or using options -B to bind directory where MPI installed into the container. Consequently, full separation of the user environment via containers is not achievable. Additionally, launching one container per process results in increased overhead.</p>
<p>Furthermore, compatibility with interconnects used for RDMA communication is necessary. In Singularity, /dev is shared with the host, so the devices themselves remain visible. However, the software stack and sockets required for device utilization must be prepared within the container. Additionally, by retrieving various information about nodes the scheduler allocated to jobs from environment variables, and passing them to the actual MPI finally enables multi-node MPI parallel.</p>
</section>
<section id="multinode-parallel">
<span id="id9"></span><h3><span class="section-number">6.5.3. </span>Multi-node parallel on FUGAKU<a class="headerlink" href="#multinode-parallel" title="Link to this heading">¶</a></h3>
<p>When creating and executing an image that incorporates MPI included in TCS by Fujitsu, the necessary steps are as follows. In the image creation section, we installed TCSDS from a directory that was mounted. However, since a network accessible repository is also available, we will use it here. When describing only the part of the definition file that prepares the execution environment, it looks as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Bootstrap</span><span class="p">:</span> <span class="n">docker</span>
<span class="n">From</span><span class="p">:</span> <span class="n">redhat</span><span class="o">/</span><span class="n">ubi8</span><span class="p">:</span><span class="mf">8.9</span>
<span class="o">%</span><span class="n">files</span>
<span class="o">%</span><span class="n">post</span>
  <span class="n">echo</span> <span class="s1">&#39;timeout=14400&#39;</span> <span class="o">&gt;&gt;</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">yum</span><span class="o">.</span><span class="n">conf</span>
  <span class="n">dnf</span> <span class="o">--</span><span class="n">noplugins</span> <span class="o">--</span><span class="n">releasever</span><span class="o">=</span><span class="mf">8.9</span> <span class="o">-</span><span class="n">y</span> <span class="n">install</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">dl</span><span class="o">.</span><span class="n">fedoraproject</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">pub</span><span class="o">/</span><span class="n">epel</span><span class="o">/</span><span class="n">epel</span><span class="o">-</span><span class="n">release</span><span class="o">-</span><span class="n">latest</span><span class="o">-</span><span class="mf">8.</span><span class="n">noarch</span><span class="o">.</span><span class="n">rpm</span>
  <span class="c1"># BEGIN Fujitsu TCS Part</span>
  <span class="n">cat</span> <span class="o">&lt;&lt;</span><span class="n">EOF</span> <span class="o">&gt;</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">yum</span><span class="o">.</span><span class="n">repos</span><span class="o">.</span><span class="n">d</span><span class="o">/</span><span class="n">fugaku</span><span class="o">.</span><span class="n">repo</span>
<span class="p">[</span><span class="n">FUGAKU</span><span class="o">-</span><span class="n">AppStream</span><span class="p">]</span>
<span class="n">name</span><span class="o">=</span><span class="n">FUGAKU</span><span class="o">-</span><span class="n">AppStream</span>
<span class="n">baseurl</span><span class="o">=</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="mf">10.4.38.1</span><span class="o">/</span><span class="n">pxinst</span><span class="o">/</span><span class="n">repos</span><span class="o">/</span><span class="n">FUGAKU</span><span class="o">/</span><span class="n">AppStream</span>
<span class="n">enabled</span><span class="o">=</span><span class="mi">1</span>
<span class="n">gpgcheck</span><span class="o">=</span><span class="mi">0</span>
<span class="p">[</span><span class="n">FUGAKU</span><span class="o">-</span><span class="n">BaseOS</span><span class="p">]</span>
<span class="n">name</span><span class="o">=</span><span class="n">FUGAKU</span><span class="o">-</span><span class="n">BaseOS</span>
<span class="n">baseurl</span><span class="o">=</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="mf">10.4.38.1</span><span class="o">/</span><span class="n">pxinst</span><span class="o">/</span><span class="n">repos</span><span class="o">/</span><span class="n">FUGAKU</span><span class="o">/</span><span class="n">BaseOS</span>
<span class="n">enabled</span><span class="o">=</span><span class="mi">1</span>
<span class="n">gpgcheck</span><span class="o">=</span><span class="mi">0</span>
<span class="n">EOF</span>
  <span class="n">dnf</span> <span class="o">--</span><span class="n">noplugins</span> <span class="o">--</span><span class="n">releasever</span><span class="o">=</span><span class="mf">8.9</span> <span class="o">-</span><span class="n">y</span> <span class="n">install</span> <span class="n">xpmem</span> <span class="n">libevent</span> <span class="n">tcl</span> <span class="n">less</span> <span class="n">hwloc</span> <span class="n">openssh</span><span class="o">-</span><span class="n">clients</span> <span class="n">gcc</span><span class="o">-</span><span class="n">c</span><span class="o">++</span> <span class="n">elfutils</span><span class="o">-</span><span class="n">libelf</span><span class="o">-</span><span class="n">devel</span> <span class="n">FJSVpxtof</span> <span class="n">FJSVpxple</span> <span class="n">FJSVpxpsm</span> <span class="n">FJSVpxkrm</span> <span class="n">FJSVxoslibmpg</span> <span class="n">papi</span><span class="o">-</span><span class="n">devel</span>
  <span class="n">dnf</span> <span class="o">--</span><span class="n">noplugins</span> <span class="n">clean</span> <span class="nb">all</span>
  <span class="c1"># END Fujitsu TCS Part</span>
</pre></div>
</div>
<p>You can install the necessary packages with this. Let’s try running the binary of a MPI sample code within the container for inter-node parallel execution. In the following example, we submit it as an interactive job with two nodes to verify its functionality.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pjsub</span> <span class="o">--</span><span class="n">interact</span> <span class="o">--</span><span class="n">sparam</span> <span class="n">wait</span><span class="o">-</span><span class="n">time</span><span class="o">=</span><span class="mi">30</span> <span class="o">-</span><span class="n">L</span> <span class="s2">&quot;rscunit=rscunit_ft01,rscgrp=int,node=2,elapse=0:30:00&quot;</span>
</pre></div>
</div>
<p>First, for the MPI binaries created in the ‘Fugaku’ environment, the runtime libraries included in TCS are necessary. Therefore, we will bind the host-side directory /opt/FJSVxtclanga using the -B option or set it as an environment variable SINGULARITY_BIND. Additionally, the necessary sockets for communication will be created in /run and /var/opt/FJSVtcs, so we’ll also incorporate those. Furthermore, information about CPU core affinity (assignment) resides in /sys/devices/system/cpu. While Singularity shares /sys by default with remounting, it lead to missing this configuration information. Similarly, cgroup information faces a similar situation, so we’ll include all of these collectively.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">SINGULARITY_BIND</span><span class="o">=</span><span class="s1">&#39;/opt/FJSVxtclanga,/var/opt/FJSVtcs,/run,/sys/devices/system/cpu,/sys/fs/cgroup&#39;</span>
<span class="ow">or</span>

<span class="n">mpiexec</span> <span class="o">-</span><span class="n">np</span> <span class="mi">2</span> <span class="n">singularity</span> <span class="n">exec</span> <span class="o">-</span><span class="n">B</span> <span class="s1">&#39;/opt/FJSVxtclanga,/var/opt/FJSVtcs,/run,/sys/devices/system/cpu,/sys/fs/cgroup&#39;</span> <span class="n">sample</span><span class="o">.</span><span class="n">sif</span> <span class="o">./</span><span class="n">appbin</span>
</pre></div>
</div>
<p>Singularity generally imports almost all host-side environment variables at runtime, but there are a few exceptions. The most notable ones are PATH and LD_LIBRARY_PATH. Therefore, the libraries located in the directory /opt/FJSVxtclanga where you’ve bound runtime won’t be accessible. To address this, you have two options during image creation. Embed these environment variables directly into %environment. Or retrieve them from other variables within %runscript.Here’s an example of how it can be done:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>%environment
  export LD_LIBRARY_PATH=/opt/FJSVxtclanga/tcsds-1.2.39/lib64:$LD_LIBRARY_PATH
or

%runscript
  export LD_LIBRARY_PATH=$FJSVXTCLANGA/lib64:$LD_LIBRARY_PATH
  export PATH=$FJSVXTCLANGA/bin:$PATH
  application
</pre></div>
</div>
<p>Or, you can pass them by the –env option, like this.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>mpiexec -np 2 singularity exec --env LD_LIBRARY_PATH=$LD_LIBRARY_PATH sample.sif ./appbin
</pre></div>
</div>
<p>The standard output and error output are not displayed on the screen, but are saved in the directory output.&lt;JOBID&gt; in your home directory. For this testing purpose, we used a sample code that outputs information for each rank.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[a0XXXX@f34-0008c ~] mpifcc -o sample sample.c
[a0XXXX@f34-0008c ~] export SINGULARITY_BIND=&#39;/opt/FJSVxtclanga,/var/opt/FJSVtcs,/run,/sys/devices/system/cpu,/sys/fs/cgroup&#39;
[a0XXXX@f34-0008c ~] mpiexec -n 2 singularity exec --env LD_LIBRARY_PATH=$LD_LIBRARY_PATH sample.sif ./sample
[a0XXXX@f34-0008c ~] logout

login1$ cd output.30999999/0/1/
login1$ ls -l
total 8
-rw-r--r-- 1 a0XXXX rccs-aot   65 Feb  1 14:59 stdout.1.0
-rw-r--r-- 1 a0XXXX rccs-aot   65 Feb  1 14:59 stdout.1.1
login1$ cat stdout.1.0
Hello world from processor f34-0008c, rank 0 out of 2 processors
login1$ cat stdout.1.1
Hello world from processor f34-0000c, rank 1 out of 2 processors
</pre></div>
</div>
<p>In this section, we extracted only the necessary conditions for multi-node MPI parallel in the FUGAKU environment to evaluate the functionality.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Singularity on FUGAKU</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Preface.html">1. Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="Preface.html#id2">2. About this document</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">3. What is Singularity?</a></li>
<li class="toctree-l1"><a class="reference internal" href="GettingStarted.html">4. Procedure for using Singularity</a></li>
<li class="toctree-l1"><a class="reference internal" href="Creating.html">5. Creating an image</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">6. Runtime options and behavior</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">6.1. Incorporating the environment on the host side</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">6.2. Instantiation and interactive processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id6">6.3. Direct execution with specified repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="#overlay">6.4. Overlay option</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mpi">6.5. MPI parallelism</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Submitting.html">7. Cooperation with job scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="Appendix.html">8. FUGAKU specific topics</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="Creating.html" title="previous chapter"><span class="section-number">5. </span>Creating an image</a></li>
      <li>Next: <a href="Submitting.html" title="next chapter"><span class="section-number">7. </span>Cooperation with job scheduler</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2021-, RIKEN R-CCS.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 8.2.3.
    &nbsp;&nbsp;<a href="https://www.fugaku.r-ccs.riken.jp/">Fugaku Potal Top</a>
    </div>
  </body>
</html>