<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. Runtime options and behavior &#8212; Singularity on FUGAKU  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=12dfc556" />
    <script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="5. Cooperation with job scheduler" href="Submitting.html" />
    <link rel="prev" title="3. Creating an image" href="Creating.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="id1">
<h1><span class="section-number">4. </span>Runtime options and behavior<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h1>
<p>Here, we will introduce some options that are often used when running Singularity, and some useful behaviors.</p>
<section id="id2">
<h2><span class="section-number">4.1. </span>Incorporating the environment on the host side<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h2>
<section id="id3">
<h3><span class="section-number">4.1.1. </span>Handling of environment variables<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<p>We have seen that access to files in your home directory is enabled by default. What about environment variables? If you display the environment variables in the container as follows, you can see that the variables on the host side at runtime are inherited.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec ubuntu2004.sif env
</pre></div>
</div>
<p>The content of %environment described in the definition file is actually posted as a script in the image, and the operation that is sourced when the container is started. Therefore, it is possible to capture and manipulate variables on the host side or overwrite them. Also, if you set a variable called SINGULARITYENV_*** (*** is an arbitrary string that can be used for variables) on the host side before executing the container, since the variable *** will be set in the container, the variable in the container is set in advance without interfering with the environment on the host side. And the content described in %environment is temporarily set or overwritten without recreating the image.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat example.def
&lt;&lt;..snip..&gt;&gt;
%environment
    export WORKFILE=/tmp/work_defined_in_env
%runscript
    echo &#39;WORKFILE in Container&#39; = ${WORKFILE}

$ cat job.sh
&lt;&lt;..snip..&gt;&gt;
    export WORKFILE=/workdir/workfile
    export SINGULARITYENV_WORKFILE=/workdir/work_defined_by_SINGULARITYENV
    ~/example.sif
</pre></div>
</div>
<p>The execution result in this case is as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">WORKFILE</span> <span class="ow">in</span> <span class="n">Container</span> <span class="o">=</span> <span class="o">/</span><span class="n">workdir</span><span class="o">/</span><span class="n">work_defined_by_SINGULARITYENV</span>

<span class="c1"># without setting SINGULARITYENV</span>
<span class="n">WORKFILE</span> <span class="ow">in</span> <span class="n">Container</span> <span class="o">=</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">work_defined_in_env</span>

<span class="c1"># without setting both of SINGULARITYENV and %environment</span>
<span class="n">WORKFILE</span> <span class="ow">in</span> <span class="n">Container</span> <span class="o">=</span> <span class="o">/</span><span class="n">workdir</span><span class="o">/</span><span class="n">workfile</span>
</pre></div>
</div>
<p>This makes it easy to change conditions in different environments without having to recreate the image.</p>
</section>
<section id="id4">
<h3><span class="section-number">4.1.2. </span>Directory binding<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h3>
<p>When you start a singularity container, it binds your own home directory, /tmp, /var/tmp, /dev, /proc, /sys. inside the container by default. It also binds the current directory at startup, but only if a directory with the same name exists in the container. If you boot the image from a directory that does not exist in the container, you may want to access other directories from within the container, as well as the current directory. For example, applications and data already installed on the host side may be redundant or huge to be imported into the image, or you may want to use the home directory of another user who shares the file.</p>
<p>In this case, you can specify additional directories to bind with the -B option at boot time. you can specify different paths by separating the directory on the host side and the mount point in the container with ':' , but if you omit the mount destination, it will be bound to the directory with the same name.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec ubuntu2004.sif -B /worktmp:/scratch ls -al /scratch
</pre></div>
</div>
<p>It will be remounted inside the container with the -bind option, but you can add more mount options here with ':' . For example, if you want to make it read-only in a container, you can write as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec ubuntu2004.sif -B /system/reference:/opt/data:ro ls -al /opt/data
</pre></div>
</div>
<p>Also, if you want to specify multiple directories, you can use multiple -B or separate them with ','.</p>
<p>There are some caveats to the binding behavior. The current directory is bound by default, so if you start a container from a directory such as /usr/ bin or /usr/lib, it will replace them in the container with the directory on the host side, resulting in a runtime error. There is. also, if you start from another user’s home directory and try to bind automatically, it will not be bound because that directory does not exist in the container. Must be explicitly specified with the -B option.</p>
</section>
</section>
<section id="id5">
<h2><span class="section-number">4.2. </span>Instantiation and interactive processing<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h2>
<p>So far, we have integrated the launch of the container and the launch of the application and used it so that the user can launch the application directly without being aware of the container. However, with this usage, when the started application is closed, the container will be closed at the same time. Of course, you can restart it, but even if it is a lightweight container implementation, if you repeat it many times, the overhead will be wasted. Also, can singularity not be used for service operations such as web servers? To accommodate these uses, singularity has a feature called instantiation that allows you to maintain containers.</p>
<p>Start and instantiate only the container as shown below. The last string is the instance name, which must be a name that is not covered by what is already running. Make sure it is started correctly with the list subcommand.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity instance start ubuntu2004.sif Focal
$ singularity instance list
INSTANCE NAME    PID      IP    IMAGE
Focal            12016          /home/uXXXXX/ubuntu2004.sif
</pre></div>
</div>
<p>The image is now unpacked and mounted, that is, the container is up and waiting. To run your application within this instance, specify the instance name instead of the image name, as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec instance://Focal cat /etc/os-release
</pre></div>
</div>
<p>Multiple applications can be submitted and executed on a single instance. For example, suppose you need to perform the same process on a huge number of files in a certain directory.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>for data in data.*
do
    singularity exec jammy.sif appl ${data}
done
</pre></div>
</div>
<p>While it is tempting to write such a script, the following will generally reduce the container launch overhead and thus increase throughput.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>singularity instance start jammy jammy.sif
for data in data.*
do
    singularity exec instance://jammy appl ${data}
done
</pre></div>
</div>
<p>Since you can start multiple instances simultaneously, you can perform different operations in different environments in single job.</p>
<p>To terminate running instance, use stop option.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity instance stop Focal
</pre></div>
</div>
</section>
<section id="id6">
<h2><span class="section-number">4.3. </span>Direct execution with specified repository<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h2>
<p>Recall that we were getting a docker image when we created the sif image. during pull and build, the layer files that make up the docker image are cached under ~/.singularity. The behavior of this cache does not change even if you specify oras or library as the target. And singularity can do this directly.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec docker://ubuntu:latest head -n 5 /etc/os-release
INFO:    Converting OCI blobs to SIF format
WARNING: &#39;nodev&#39; mount option set on /worktmp, it could be a source of failure during build process
INFO:    Starting build...
Getting image source signatures
Copying blob a39c84e173f0 done
Copying config 4db8450a4d done
Writing manifest to image destination
Storing signatures
2021/03/21 12:14:15  info unpack layer: sha256:a39c84e173f038958d338f55a9e8ee64bb6643e8ac6ae98e08ca65146e668d86
2021/03/21 12:14:15  warn xattr{etc/gshadow} ignoring ENOTSUP on setxattr &quot;user.rootlesscontainers&quot;
2021/03/21 12:14:15  warn xattr{/worktmp/build-temp-063655636/rootfs/etc/gshadow} destination filesystem does not support xattrs, further warnings will be suppressed
INFO:    Creating SIF file...
NAME=&quot;Ubuntu&quot;
VERSION=&quot;20.04.3 LTS (Focal Fossa)&quot;
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=&quot;Ubuntu 20.04.3 LTS&quot;
</pre></div>
</div>
<p>As you can see from the message, it is internally sif imaged and stored under ~/.singularity. when you re-execute, it will check the repository for updates, make the best use of the cache, and synchronize. If there is no update, launch his sif image in the cache directly as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec docker://ubuntu:latest head -n 5 /etc/os-release
INFO:    Using cached SIF image
NAME=&quot;Ubuntu&quot;
VERSION=&quot;20.04.3 LTS (Focal Fossa)&quot;
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=&quot;Ubuntu 20.04.3 LTS&quot;
</pre></div>
</div>
<p>If there is an update on the repository side, get it, recreate the sif image, and then execute it. This execution method is not suitable when it is necessary to maintain an execution environment with a proven track record. It is recommended that you always use it only when you want to import and use the latest version of the repository.</p>
<p>You can also view cache information and clear cache files.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity cache list -v
NAME                     DATE CREATED           SIZE             TYPE
061be60b872e3a155a2364   2021-10-28 22:14:15    0.34 KiB         blob
2d5d59c100e7ad4ac35a22   2021-11-03 13:54:19    78.57 MiB        blob
&lt;&lt;..snip..&gt;&gt;
sha256.9a6ee1f8fdecb21   2023-02-22 17:14:02    2.65 MiB         library
626ffe58f6e7566e00254b   2021-10-28 22:14:18    25.15 MiB        oci-tmp

There are 2 container file(s) using 27.81 MiB and 25 oci blob file(s) using 692.86 MiB of space
Total space used: 720.67 MiB

$ singularity cache clean
</pre></div>
</div>
<p>These caches are used not only at runtime, but also at build/pull time.</p>
</section>
<section id="overlay">
<h2><span class="section-number">4.4. </span>Overlay option<a class="headerlink" href="#overlay" title="Link to this heading">¶</a></h2>
<p>The SIF image entity is SquashFS(almost tar+gzip), so it is read-only at runtime. However, there are cases when you want to temporarily place files in the image, or when you need an image that is only partially different, or when you want to have only the differences because having multiple images that contain everything is a waste of capacity. Singularity provides 2 solutions.</p>
<ol class="arabic simple">
<li><p>The –writable-tmpfs option that makes memory-derived tmpfs temporarily writable by superimposing them on the read-only image.</p></li>
<li><p>The –overlay option that prepares a separate persistent writable file and overlays it at runtime.</p></li>
</ol>
<p>The writable-tmpfs option superimposes tmpfs on the existing image, so new files and directories can be created at any location in container. However, files originally in the image cannot be manipulated with causing the error of Permission denied. Also, the changes will only exist in tmpfs, so they will disappear after the container is terminated. If you have necessary data or files, save them separately before exiting the container.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity shell -writable-tmpfs something.sif
Singularity&gt; echo hogefuga &gt; /etc/testfile
Singularity&gt; ls -al /etc/testfile
-rw-rw-r-- 1 uXXXXX fugaku 9 Feb 14 04:50 /etc/testfile
Singularity&gt; echo hogefuga &gt; /etc/os-release
bash: /etc/os-release: Permission denied
Singularity&gt; exit

$ singularity shell something.sif
Singularity&gt; ls -al /etc/hogefuga
ls: cannot access &#39;/etc/hogefuga&#39;: No such file or directory
</pre></div>
</div>
<p>The –overlay function, which overlays a separately prepared persistent writable file at runtime, requires a separate image file to be created instead of tmpfs. If the same file exists in both SIF and Overlay, the overlay side has priority. This means that it enables partial modification of the image.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># 1GBの overlay イメージを作成し、指定したディレクトリを予め作成しておく。
# Creating a 1GB of image file with prepared directory inside.
$ singularity overlay create --size 1024 --create-dir /usr/share/apps overlay.img

# overlay イメージ内にデータを展開
# Extracting data into Overlay image.
$ singularity exec --overlay overlay.img core.sif tar xzf reference-data.tar.gz -C /usr/share/apps

# overlay イメージをコンテナに重ね合わせて実行
# Executeing container with the overlay image.
$ singularity exec --overlay overlay.img core.sif ls -al /usr/share/apps
</pre></div>
</div>
<p>The written data remains in overlay.img and can be used to carry and segregate your data. However, please note that the image file is formatted in ext3 and the owner information is also retained as well, so be careful when using it on a different system. On the other hand, the image file can be used on both of AARCH64 and x86_64 by singularity. It is also possible to directly manipulate the image by mounting it as loopback, or to resize it in the same way as a normal partition, as shown below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ e2fsck -f overlay.img &amp;&amp; resize2fs overlay.img 4096M
</pre></div>
</div>
<p>In addition, you can integrate the overlay image into a SIF image to be a single file by means of singularity sif command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity sif list ubuarm20.sif
------------------------------------------------------------------------------
ID   |GROUP   |LINK    |SIF POSITION (start-end)  |TYPE
------------------------------------------------------------------------------
1    |1       |NONE    |65536-65574               |Def.FILE
2    |1       |NONE    |131072-131169             |JSON.Generic
3    |1       |NONE    |196608-26320896           |FS (Squashfs/*System/arm64)

$ singularity sif add --datatype 4 --partfs 2 --parttype 4 --partarch 4 ubuarm20.sif overlay.img
$ singularity sif list ubuarm20.sif
------------------------------------------------------------------------------
ID   |GROUP   |LINK    |SIF POSITION (start-end)  |TYPE
------------------------------------------------------------------------------
1    |1       |NONE    |65536-65574               |Def.FILE
2    |1       |NONE    |131072-131169             |JSON.Generic
3    |1       |NONE    |196608-26320896           |FS (Squashfs/*System/arm64)
4    |NONE    |NONE    |26320896-1100062720       |FS (Ext3/Overlay/arm64)
</pre></div>
</div>
<p>To see the meaning of each parameter, please refer to the manual which you can see by the command singularity sif add –help. This function allows for the operation of images whose contents can be modified. Later, these additions can be taken out or deleted.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity sif dump 4 ubuarm20.sif  &gt; ovl.img
$ singularity sif del 4 ubuarm20.sif
</pre></div>
</div>
<p>These operations can also be performed on the login node, so there is no need to submit a job for this purpose. Overlay also contributes to reducing the metadata access load on the shared file system, especially when there are many small files, since I/O is completed within a single file.</p>
</section>
<section id="mpi">
<h2><span class="section-number">4.5. </span>About mpi parallel<a class="headerlink" href="#mpi" title="Link to this heading">¶</a></h2>
<p>MPI 並列については、ノード内での並列までは大きな問題がありませんが、マルチノード並列では解決すべき問題が多岐に渡るため、イメージの作成と実行時に追加で作業や設定が必要です。</p>
<section id="id7">
<h3><span class="section-number">4.5.1. </span>Parallel within a node<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h3>
<p>In parallel within a node, the process manager launches the application directly, so processing does not extend outside the container. While launching the container with singularity, launch his mpi application with mpirun (mpiexec) etc. of course, it is assumed that the required mpi runtime is in the container. An example command looks like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ singularity exec mpi-apps.sif mpiexec -np 8 ~/myapps/hoge inputfile
</pre></div>
</div>
<p>Only one container is started, and multiple processes run in it.</p>
</section>
<section id="id8">
<h3><span class="section-number">4.5.2. </span>マルチノード並列の概要<a class="headerlink" href="#id8" title="Link to this heading">¶</a></h3>
<p>マルチノード並列でノード内での並列と同様に起動すると、他のノード上でのプロセスマネージャの起動は ssh や PMI により行われます。しかしその先はコンテナ環境が立ち上がっているわけではないため、アプリケーションの起動に失敗します。そのため、MPI のプロセスマネージャにコンテナ起動を含めてやらせてしまうことでこれに対処します。起動の仕方のイメージは以下のようになります。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ mpiexec -np 32 --machinefile nodefile singularity exec mpi-apps.sif ~/myapps/hoge inputfile
</pre></div>
</div>
<p>ノード内並列のものと比べ、mpiexec と singularity コマンドの順番が違っていることが分かると思います。この場合、mpiexec を Sgingularity コンテナ外で実行する必要があり、実行時にホスト側とコンテナ内で同じ MPI をインストールないしは -B 等で共有した状態が必要になります。そのため、コンテナによるユーザー環境の分離が十分にはできません。また、プロセス数分のコンテナが起動されるため、その分オーバーヘッドも大きくなります。</p>
<p>さらに、RDMA 通信に用いられるインターコネクタの対応が必要です。Singularity では /dev がホスト側と共有されるので、デバイスそのものが見えないことはありません。しかし、デバイスを利用するのに必要なソフトウエアスタックやソケットについてはコンテナ内に用意する必要があります。また、スケジューラによりジョブに割り当てられたノードの情報等を環境変数から取得し、実際の MPI に渡すことでようやくマルチノードの MPI 並列が実現します。</p>
</section>
<section id="id9">
<h3><span class="section-number">4.5.3. </span>「富岳」環境でのマルチノード並列<a class="headerlink" href="#id9" title="Link to this heading">¶</a></h3>
<p>富士通による TCS に含まれる MPI を取り込んだイメージを作成し、実行する際に必要な方法は次のようになります。イメージ作成の章ではマウントされている領域にあるディレクトリから TCSDS をインストールしましたが、ネットワークアクセスのできるリポジトリも用意されたため、こちらを利用します。 定義ファイルのうち、実行環境を準備する部分だけを記述すると、以下のようになります。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Bootstrap</span><span class="p">:</span> <span class="n">docker</span>
<span class="n">From</span><span class="p">:</span> <span class="n">redhat</span><span class="o">/</span><span class="n">ubi8</span><span class="p">:</span><span class="mf">8.8</span>
<span class="o">%</span><span class="n">files</span>
<span class="o">%</span><span class="n">post</span>
  <span class="n">dnf</span> <span class="o">--</span><span class="n">noplugins</span> <span class="o">--</span><span class="n">releasever</span><span class="o">=</span><span class="mf">8.8</span> <span class="o">-</span><span class="n">y</span> <span class="n">install</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">dl</span><span class="o">.</span><span class="n">fedoraproject</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">pub</span><span class="o">/</span><span class="n">epel</span><span class="o">/</span><span class="n">epel</span><span class="o">-</span><span class="n">release</span><span class="o">-</span><span class="n">latest</span><span class="o">-</span><span class="mf">8.</span><span class="n">noarch</span><span class="o">.</span><span class="n">rpm</span>
  <span class="c1"># BEGIN Fujitsu TCS Part</span>
  <span class="n">cat</span> <span class="o">&lt;&lt;</span><span class="n">EOF</span> <span class="o">&gt;</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">yum</span><span class="o">.</span><span class="n">repos</span><span class="o">.</span><span class="n">d</span><span class="o">/</span><span class="n">fugaku</span><span class="o">.</span><span class="n">repo</span>
<span class="p">[</span><span class="n">FUGAKU</span><span class="o">-</span><span class="n">AppStream</span><span class="p">]</span>
<span class="n">name</span><span class="o">=</span><span class="n">FUGAKU</span><span class="o">-</span><span class="n">AppStream</span>
<span class="n">baseurl</span><span class="o">=</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="mf">10.4.38.1</span><span class="o">/</span><span class="n">pxinst</span><span class="o">/</span><span class="n">repos</span><span class="o">/</span><span class="n">FUGAKU</span><span class="o">/</span><span class="n">AppStream</span>
<span class="n">enabled</span><span class="o">=</span><span class="mi">1</span>
<span class="n">gpgcheck</span><span class="o">=</span><span class="mi">0</span>
<span class="p">[</span><span class="n">FUGAKU</span><span class="o">-</span><span class="n">BaseOS</span><span class="p">]</span>
<span class="n">name</span><span class="o">=</span><span class="n">FUGAKU</span><span class="o">-</span><span class="n">BaseOS</span>
<span class="n">baseurl</span><span class="o">=</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="mf">10.4.38.1</span><span class="o">/</span><span class="n">pxinst</span><span class="o">/</span><span class="n">repos</span><span class="o">/</span><span class="n">FUGAKU</span><span class="o">/</span><span class="n">BaseOS</span>
<span class="n">enabled</span><span class="o">=</span><span class="mi">1</span>
<span class="n">gpgcheck</span><span class="o">=</span><span class="mi">0</span>
<span class="n">EOF</span>
  <span class="n">dnf</span> <span class="o">--</span><span class="n">noplugins</span> <span class="o">--</span><span class="n">releasever</span><span class="o">=</span><span class="mf">8.8</span> <span class="o">-</span><span class="n">y</span> <span class="n">install</span> <span class="n">xpmem</span> <span class="n">libevent</span> <span class="n">tcl</span> <span class="n">less</span> <span class="n">hwloc</span> <span class="n">openssh</span><span class="o">-</span><span class="n">clients</span> <span class="n">gcc</span><span class="o">-</span><span class="n">c</span><span class="o">++</span> <span class="n">elfutils</span><span class="o">-</span><span class="n">libelf</span><span class="o">-</span><span class="n">devel</span> <span class="n">FJSVpxtof</span> <span class="n">FJSVpxple</span> <span class="n">FJSVpxpsm</span> <span class="n">FJSVpxkrm</span> <span class="n">FJSVxoslibmpg</span> <span class="n">papi</span><span class="o">-</span><span class="n">devel</span>
  <span class="n">dnf</span> <span class="o">--</span><span class="n">noplugins</span> <span class="n">clean</span> <span class="nb">all</span>
<span class="c1"># END Fujitsu TCS Part</span>
</pre></div>
</div>
<p>必要なパッケージ群はこれでインストールできます。手元にある MPI のサンプルコードのバイナリをコンテナ内でノード間並列してみましょう。
以下では、次のように2ノードのインタラクティブジョブとして投入して動作確認をしています。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pjsub</span> <span class="o">--</span><span class="n">interact</span> <span class="o">--</span><span class="n">sparam</span> <span class="n">wait</span><span class="o">-</span><span class="n">time</span><span class="o">=</span><span class="mi">30</span> <span class="o">-</span><span class="n">L</span> <span class="s2">&quot;rscunit=rscunit_ft01,rscgrp=int,node=2,elapse=0:30:00,jobenv=singularity&quot;</span>
</pre></div>
</div>
<p>まず、「富岳」環境でつくられた MPI バイナリには TCS に含まれるランタイムライブラリが必要なので、これはホスト側のディレクトリ /opt/FJSVxtclanga を -B オプションないし、環境変数 SINGULARITY_BIND に設定してバインドして使うことにします。さらに、通信に必要なソケット等が /run, /var/opt/FJSVxtclanga に作られますので、これも取り込みます。また、CPU コアのアフィニティ（割り当て）情報が /sys/devices/system/cpu にあります。Singularity は /sys はデフォルトで共有しますが、再マウントして使うことから、この設定情報を取りこぼしてしまいます。cgroup 情報も同様の状況なので、これらを一括で取り込みます。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">SINGULARITY_BIND</span><span class="o">=</span><span class="s1">&#39;/opt/FJSVxtclanga,/var/opt/FJSVxtclanga,/run,/sys/devices/system/cpu,/sys/fs/cgroup&#39;</span>
<span class="ow">or</span>

<span class="n">mpiexec</span> <span class="o">-</span><span class="n">np</span> <span class="mi">2</span> <span class="n">singularity</span> <span class="n">exec</span> <span class="o">-</span><span class="n">B</span> <span class="s1">&#39;/opt/FJSVxtclanga,/var/opt/FJSVxtclanga,/run,/sys/devices/system/cpu,/sys/fs/cgroup&#39;</span> <span class="n">sample</span><span class="o">.</span><span class="n">sif</span> <span class="o">./</span><span class="n">appbin</span>
</pre></div>
</div>
<p>Singularity は実行時にホスト側の環境変数をほぼ全て取り込みますが、いくつかの例外があります。その最たるものが PATH と LD_LIBRARY_PATH です。せっかくバインドした /opt/FJSVxtclanga にあるライブラリが使えなくなりますので、これをイメージ作成時に %environment に直接埋め込んでおくか、%runscript で他の変数から取得するように書く方法があります。以下のようになります。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>%environment
  export LD_LIBRARY_PATH=/opt/FJSVxtclanga/tcsds-1.2.38/lib64:$LD_LIBRARY_PATH
or

%runscript
  export LD_LIBRARY_PATH=$FJSVXTCLANGA/lib64:$LD_LIBRARY_PATH
  export PATH=$FJSVXTCLANGA/bin:$PATH
  application
</pre></div>
</div>
<p>もしくは、以下のように –env オプションで引き渡して追加されるようにします。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>mpiexec -np 2 singularity exec --env LD_LIBRARY_PATH=$LD_LIBRARY_PATH sample.sif ./appbin
</pre></div>
</div>
<p>標準出力及びエラー出力は画面に表示されず、ホームディレクトリの output.&lt;JOBID&gt; 以下に保存されます。テストではランクごとに情報を出力するサンプルコードを用いました。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[a0XXXX@f34-0008c ~] mpifcc -o sample sample.c
[a0XXXX@f34-0008c ~] export SINGULARITY_BIND=&#39;/opt/FJSVxtclanga,/var/opt/FJSVxtclanga,/run,/sys/devices/system/cpu,/sys/fs/cgroup&#39;
[a0XXXX@f34-0008c ~] mpiexec -n 2 singularity exec --env LD_LIBRARY_PATH=$LD_LIBRARY_PATH sample.sif ./sample
[a0XXXX@f34-0008c ~] logout

login1$ cd output.30999999/0/1/
login1$ ls -l
total 8
-rw-r--r-- 1 a0XXXX rccs-aot   65 Feb  1 14:59 stdout.1.0
-rw-r--r-- 1 a0XXXX rccs-aot   65 Feb  1 14:59 stdout.1.1
login1$ cat stdout.1.0
Hello world from processor f34-0008c, rank 0 out of 2 processors
login1$ cat stdout.1.1
Hello world from processor f34-0000c, rank 1 out of 2 processors
</pre></div>
</div>
<p>ここでは、「富岳」環境におけるノード間 MPI 並列に必要な条件だけを抽出して動作の確認をしました。</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Singularity on FUGAKU</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">1. What is Singularity?</a></li>
<li class="toctree-l1"><a class="reference internal" href="GettingStarted.html">2. Procedure for using Singularity</a></li>
<li class="toctree-l1"><a class="reference internal" href="Creating.html">3. Creating an image</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">4. Runtime options and behavior</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">4.1. Incorporating the environment on the host side</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">4.2. Instantiation and interactive processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id6">4.3. Direct execution with specified repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="#overlay">4.4. Overlay option</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mpi">4.5. About mpi parallel</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Submitting.html">5. Cooperation with job scheduler</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="Creating.html" title="previous chapter"><span class="section-number">3. </span>Creating an image</a></li>
      <li>Next: <a href="Submitting.html" title="next chapter"><span class="section-number">5. </span>Cooperation with job scheduler</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2021-, RIKEN R-CCS.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 7.2.6.
    &nbsp;&nbsp;<a href="https://www.fugaku.r-ccs.riken.jp/">Fugaku Potal Top</a>
    </div>
  </body>
</html>